# LA组入门攻略
## Part0: 写在前面

## Part1: NLP基础
* CS224n
  * [2021课程主页](http://web.stanford.edu/class/cs224n/)
  * [2019课程录像](https://www.bilibili.com/video/BV1Eb411H7Pq?from=search&seid=14373694631452542823)
  * 学有余力请阅读Suggested Readings中，各个算法的原论文
  * 完成五个Project
    * [参考答案](./Part1/参考答案)
  * [历年总结课件](./Part1/总结课件)

## Part2: [LA经典论文推荐](https://docs.qq.com/sheet/DVFl0eUlvaXpPdXZR)
* 详细介绍可以参考标题文档
* LA组
  * [CoSDA-ML: Multi-Lingual Code-Switching Data Augmentation for Zero-Shot Cross-Lingual NLP](https://www.ijcai.org/proceedings/2020/0533.pdf)
  * [A Survey on Spoken Language Understanding: Recent Advances and New Frontiers](https://arxiv.org/abs/2103.03095)
  * [A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding](https://aclanthology.org/D19-1214/)
  * [A Distributed Representation-Based Framework for Cross-Lingual Transfer Parsing](http://people.csail.mit.edu/jiang_guo/papers/jair2016-clnndep.pdf)
  * [Knowledge Graph Grounded Goal Planning for Open-Domain Conversation Generation](http://ir.hit.edu.cn/~jxu/jun_files/papers/AAAI2020-Jun%20Xu-KnowHRL.pdf)
  * [From static to dynamic word representations: a survey](http://ir.hit.edu.cn/~car/papers/icmlc2020-wang.pdf)
  * [Consistency Regularization for Cross-Lingual Fine-Tuning](https://arxiv.org/abs/2106.08226)
  * [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740)
  * [Sequence-to-sequence data augmentation for dialogue language understanding](https://arxiv.org/abs/1807.01554)
  * [Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network](https://atmahou.github.io/attachments/atma's_acl2020_FewShot.pdf )
* 非LA组
  * [Deep Biaffine Attention for Neural Dependency Parsing](https://arxiv.org/abs/1611.01734)
  * [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
  * [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)
  * [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://aclanthology.org/N19-1423/)
  * [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/forum?id=r1xMH1BtvB)
  * [Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference](https://aclanthology.org/2021.eacl-main.20/)
  * [It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners](https://aclanthology.org/2021.naacl-main.185.pdf)
  * [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
  * [Deep contextualized word representations](https://arxiv.org/abs/1409.0473)
  * [Climbing towards NLU- On Meaning, Form, and Understanding in the Age of Data](https://aclanthology.org/2020.acl-main.463/)
  * [Task-Oriented Dialogue as Dataflow Synthesis](https://arxiv.org/pdf/2009.11423.pdf)
  * [Meena-Towards a Human-like Open-Domain Chatbot](https://arxiv.org/pdf/2001.09977.pdf)
  * [A Fast and Accurate Dependency Parser using Neural Networks](https://aclanthology.org/D14-1082/)
  * [Neural architectures for named entity recognition](https://aclanthology.org/N16-1030.pdf)
  * [Document Modeling with Gated Recurrent Neural Network for Sentiment Classification](https://aclanthology.org/D15-1167.pdf)
  * [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)
  * [Language Models as Knowledge Bases?](https://arxiv.org/pdf/1909.01066.pdf)
  * [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
  * [POMDP-based Statistical Spoken Dialogue Systems: a Review](https://www.microsoft.com/en-us/research/publication/pomdp-based-statistical-spoken-dialogue-systems-a-review/)
  * [Confident Learning: Estimating Uncertainty in Dataset Labels](http://www.researchgate.net/publication/337005918_Confident_Learning_Estimating_Uncertainty_in_Dataset_Labels)
  * [Learning Active Learning from Data](https://papers.nips.cc/paper/2017/file/8ca8da41fe1ebc8d3ca31dc14f5fc56c-Paper.pdf)
  * [Neural machine translation of rare words with subword units.](https://www.aclweb.org/anthology/P16-1162.pdf)
  * [A Simple Framework for Contrastive Learning of Visual Representations](https://static.aminer.cn/storage/pdf/arxiv/20/2002/2002.05709.pdf)
  * [How Powerful are Graph Neural Networks?](https://openreview.net/forum?id=ryGs6iA5Km&noteId=rkl2Q1Qi6X&noteId=rkl2Q1Qi6X)
  * [Vocabulary Learning via Optimal Transport for Machine Translation](https://arxiv.org/abs/2012.15671)
  * [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)
  * [Graph Attention Networks](https://arxiv.org/abs/1710.10903)
  * [ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](https://arxiv.org/abs/1908.02265)
  * [Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks](https://arxiv.org/abs/2004.06165)
  * [UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning](https://arxiv.org/abs/2012.15409)
  * [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
  * [Distributed Representations of Words and Phrases and their Compositionality](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
  * [Transition-Based Dependency Parsing with Stack Long Short-Term Memory](https://aclanthology.org/P15-1033/)
  * [Unsupervised Cross-lingual Representation Learning at Scale](https://aclanthology.org/2020.acl-main.747/)
  * [Word Translation Without Parallel Data](https://arxiv.org/abs/1710.04087)
  * [MultiWOZ -- A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling](https://arxiv.org/abs/1810.00278)
  * [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536)
  * [Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210)
  * [Neural Reading Comprehension and Beyond](https://www.cs.princeton.edu/~danqic/papers/thesis.pdf)
  * [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
  * [Bidirectional Attention Flow for Machine Comprehension](https://arxiv.org/abs/1611.01603)
  * [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114)
  * [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)
  * [Pointer Networks](https://arxiv.org/abs/1506.03134)
  * [Neural Transfer Learning for Natural Language Processing](https://aran.library.nuigalway.ie/bitstream/handle/10379/15463/neural_transfer_learning_for_nlp.pdf?sequence=1&isAllowed=y)
  * [Generating Sentences from a Continuous Space](https://arxiv.org/pdf/1511.06349.pdf)
  * [Sequence-Level Knowledge Distillation](https://arxiv.org/pdf/1606.07947.pdf?__hstc=36392319.43051b9659a07455a3db8391a8f20ea4.1480118400085.1480118400086.1480118400087.1&__hssc=36392319.1.1480118400088&__hsfp=528229161)
  * [Model-agnostic meta-learning for fast adaptation of deep networks](http://proceedings.mlr.press/v70/finn17a/finn17a.pdf)
  * [Matching Networks for One Shot Learning](https://arxiv.org/abs/1606.04080)
  * [Conditional random fields: Probabilistic models for segmenting and labeling sequence data](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers)
  * [Coarse-to-Fine Decoding for Neural Semantic Parsing](https://aclanthology.org/P18-1068.pdf)
  * [A Syntactic Neural Model for General-Purpose Code Generation](https://aclanthology.org/P17-1041.pdf)
  * [Compositional Semantic Parsing on Semi-Structured Tables](https://aclanthology.org/P15-1142.pdf)
  * [Encode, Tag, Realize: High-Precision Text Editing](https://arxiv.org/pdf/1909.01187.pdf)
  * [SpellGCN: Incorporating Phonological and Visual Similarities into Language Models for Chinese Spelling Check](https://arxiv.org/pdf/2004.14166.pdf)
  * [Levenshtein Transformer](https://arxiv.org/abs/1905.11006)

## Part3: [模型框架](./Part3/BiLSTM-Seqlabeling)
* POS Tagging任务
* 基于BiLSTM
* 包含了数据处理、搭建、训练、测试的完整过程
* 对基于深度学习的NLP框架有一个大体的认识，为完成Part4做准备

## Part4: 项目实践
* 任务待定
  * 暂定NER和Graph-based Paser
  * 基于非预训练模型（#）
  * 基于预训练模型（#）
  * 计算资源？

## 其他
* 机器学习基础
  * 吴恩达
* 一些推荐的博客/github库
